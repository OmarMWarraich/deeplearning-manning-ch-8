{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers:\n",
    "    Text generation with LSTM\n",
    "    Implementing DeepDream\n",
    "    Performing neural style transfer\n",
    "    Variational autoencoders\n",
    "    Understanding generative adversarial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Text generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnns used to generate sequence data. e.g., text generation, same techniques can be genralized to any kind of\n",
    "sequence data: applicaple to a sequences of musical notes in order to generate new music, to timeseries of brush\n",
    "-stroke data(e.g., recorded while an arties paints on an iPad) to generate paintaings stroke by stroke and so on.\n",
    "Sequence data gen not limited to aritstic content generation. usable in speech synthesis and dialogue generation\n",
    "for chatbots. The Smart Reply feature Google 2016\n",
    "\n",
    "character-level neural language model: The output of the model will be a softmax over all possible characters: a \n",
    "        probability distribution for the next character. e.g., take a LSTM layer, feed it strings of N characters\n",
    "        extracted from a text corpus, and train it to predict character N + 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3 The importance of the sampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy sampling = a naive aprroach; always choosing the most likely next element. results are repititive, predictable strings unlike \n",
    "                  coherent language.\n",
    "stochastic sampling = makes slightly more surprising choices: introduces randomness in the sampling process,\n",
    "                  by sampling from the probability distribution for the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing8.1 Reweighting a probability distribution to a different temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Given a temperature value, a new probability distribution is computed from the original one(the softmax output of the model)\n",
    "###by reweighting in the following way\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5): # original distribution is a 1D Numpy array of\n",
    "    distribution = np.log(original_distribution) / temperature     # probability values that must sum to 1. temperature \n",
    "    distribution = np.exp(distribution)                   # is a factor quantifying the entropy of the output distribution.\n",
    "    return distribution / np.sum(distribution)   # Returns a reweighted version of the original distribution. The sum of \n",
    "                        # distribution may no longer be 1, so divide it by its sum to obtain the new distribution.\n",
    "    \n",
    "# Higher temps result in sampling distributions of higher entropy that will genreate more surprising and unstructured\n",
    "# generated data, whereas a lower temp will result in less randomness and much more predicted generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.4 Implementing character-level LSTM text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.2: Downloading and parsing the initial text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.3: vectorizing sequence of characers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract partially overlapping sequences of length maxlen, one-hot encode them, and pack them in a 3D Numpy array x\n",
    "of shape (sequences, maxlen, unique_characters). prepare an array y simulataneously containing the corresponding\n",
    "targets: the one-hot-encoded characters that come after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 60   # Extract sequences of 60 characters\n",
    "step = 3      # Sample a new sequence every three characters\n",
    "sentences = [] # Holds the extracted sequences\n",
    "next_chars = [] # Holds the targets(the follow-up characters)  \n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "chars = sorted(list(set(text)))    # list of unique characters in the corpus\n",
    "print('Unique characters:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)  # Dictionary that maps unique characters to their\n",
    "                                                                  # index in the list \"chars\"  \n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)   # One hot encodes\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)           # the character  \n",
    "for i, sentence in enumerate(sentences):                            # into\n",
    "    for t, char in enumerate(sentence):                             # binary\n",
    "        x[i, t, char_indices[char]] = 1                             # arrays\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING THE NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single LSTM layer followed by a dense classifier and softmax over all possible characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.4: Single layer LSTM model for next-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoded targets, therefore, categorical_crossentropy used as the loss to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.5: Model compilation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE LANGUAGE MODEL AND SAMPLING FROM IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draw from the model a probability distribution for the next character, given the genrated text available so far.\n",
    "2. Reweight the distribution to a certain temperature.\n",
    "3. Sample the next character at random according to the reweighted distribution.\n",
    "4. Add the new character at the end of the available text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.6: Function to sample the next character given the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code used to reweight the original probability distribution coming out of the model and draw a character index from it.\n",
    "# (the sampling function)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.7: Text-generation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):     # Trains the model for 60 epochs\n",
    "    print('epochs', epoch)\n",
    "    model.fit(x, y, batch_size=128, epochs=1)   # Fits the model for one iteration on the data\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)   # Selects a text seed at random \n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('---Generating with seed: \"' + generated_text +'\"')\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:     # Tries a range of different sampling temperatures\n",
    "        print('------temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "        \n",
    "            for i in range(400):     # Generates 400 characters, starting from the seed text\n",
    "                sampled = np.zeros((1, maxlen, len(chars)))    # One-hot encodes the characters generated so far.\n",
    "                for t, char in enumerate(generated_text):\n",
    "                    sampled[0, t, char_indices[char]] = 1.\n",
    "                    \n",
    "                preds = model.predict(sampled, verbose=0)[0]\n",
    "                next_index = sample(preds, temperature)\n",
    "                next_char = chars[next_index]\n",
    "                \n",
    "                generated_text += next_char\n",
    "                generated_text = generated_text[1:]\n",
    "                \n",
    "                sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 DeepDream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artistic image-modification technique that uses the representations learned by convolutional neural networks. \n",
    "Trained on ImageNet, where dog breeds and bird species are vastly over represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Implementing DeepDream in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.8 Loading the pretrained Inception V3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)  # Model isnt trained, so this command disables all training-specific operations.\n",
    "\n",
    "model = inception_v3.InceptionV3(weights='imagenet',  # Builds the V3 network without its convolution base. The\n",
    "                                 include_top=False)   # model will be loaded with pretrained ImageNet weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.9 Setting Up the DeepDream configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_contributions = {   # Dictionary mapping layer names to a coefficient quantifying how much the layer's \n",
    "    'mixed2': 0.2,        # activation contributes to the loass you 'll seek to maximize. Note that the layer names\n",
    "    'mixed3': 3.,         # are hardcoded in the builtin V3. All layers list using model.summary()\n",
    "    'mixed4': 2.,\n",
    "    'mixed5': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.10 Listing the loss to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])   # Creates a dict maps layer names 2 layer insts.\n",
    "\n",
    "loss = K.variable(0.)  # Define loss by adding layer contributions to this scalar variable.\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output    # Retrieves the layer's output\n",
    "    \n",
    "    scaling = K.prod(K.cast(K.Shape(activation), 'float32'))\n",
    "    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling # Adds the L2 norm of the features of\n",
    "            # a layer to the loss. Border artifacts be avoided by only involving non border pixels in the loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.11 Gradient-ascent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream = model.input      # This tensor holds the generated image: the dream.\n",
    "\n",
    "grads = K.gradients(loss, dream)[0]  # Computes the gradients of the dream with regard to the loss.\n",
    "\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)   # Normalizes the gradients (important trick)\n",
    "\n",
    "outputs = [loss, grads]                             # Sets up a Keras function to retrieve the value of the loss \n",
    "fetch_loss_and_grads = K.function([dream], outputs) # and gradients given an input image\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):  # This fn runs a grad asc 4 no(s) of iterations.\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('...Loss value at', i, ':', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.12 Running gradient ascent over different successive scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "step = 0.01          # Gradient ascent step size                        # Playing with these hyperparameters \n",
    "num_octave = 3       # No. of scales at which to run gradient ascent.   # will let you achieve new effects. \n",
    "octave_scale = 1.4   # Size ratio between scales\n",
    "iterations = 20      # Number of ascent steps to run at each scale\n",
    "\n",
    "max_loss = 10.       # If loss > 0, grad ascent process be interrupted to avoud ugly artifacts.\n",
    "\n",
    "base_image_path = '...'     # Path to the image to be used\n",
    "\n",
    "img = preprocess_image(base_image_path)    # Loads the base image into a Numpy array(fn defined next listing)\n",
    "\n",
    "original_shape = img.shape[1:3]\n",
    "successive_shapes = [original_shape]                # Prepares a list of shape tuples defining the different scales\n",
    "for i in range(1, num_octave):                      # at which to run gradient ascent.\n",
    "    shape = tuple([int(dim / (octave_scale ** i))\n",
    "        for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "    \n",
    "successive_shapes = successive_shapes[::-1]     # Reverses the list of shape so they 're in increasing order.\n",
    "    \n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0]) # Resizes Numpy array of the img to the smallest scale.\n",
    "\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape', shape)\n",
    "    img = resize_img(img, shape)      # Scales up the dream image\n",
    "    img = gradient_ascent(img,             \n",
    "                          iterations=iterations,     # Runs gradient \n",
    "                          step=step,                 # ascent\n",
    "                          max_loss=max_loss)         # altering the dream\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)   # Scales up smalled pixaled v of org img.\n",
    "    same_size_original = resize_img(original_img, shape)   # Computes the high quality v of the org img at this size.\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img # The diff btw the two is the detail lost scaling up.\n",
    "    \n",
    "    img += lost_detail   # Reinjects lost detail into the dream\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png' )\n",
    "    \n",
    "save_img(img, fname='final_dream.png')\n",
    "\n",
    "# Note: this code uses aux Numpy fns such as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.13: Auxilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "\n",
    "def preprocess_image(image_path):                       # Util fn 2 open, resize and format pictures into tensors\n",
    "    img = image.load_img(image_path)                    # that Inception V3 can process\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):                                 # Util fn: convert a tensor into a valid image.\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))      # Undoes preprocessing per by inception_v3.preprocess_input\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# Note: Org Inc V3 network trained 2 recognize concepts in imgs of size 299 x 299, and images scaled down by a \n",
    "#       reasonable factor, DeepDream impl produces much better results on images btw 300x300 & 400 x 400. however: any size be run\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepDream consists of running a convnet in reverse to generate inputs based on the representations learned by the\n",
    "network.\n",
    "The results produced are fun and somewhat similar to the visual artifacts induced in humans by the disruption of the\n",
    "visual cortex via phsychedelics.\n",
    "Process not specific to image models or even convnets. Doable for speech, music and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Neural style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consists of applying the style of a reference image to a target image while conserving the target image content.\n",
    "style means textures, colors and visual patterns in the image at various spatial scales and the content is the\n",
    "higher-level macrostructure of the image. \n",
    "define a loss fn to specify what to achieve, and minimize this loss.\n",
    "wat to achieve = conserve the content of the original image while adopting the style of the reference image\n",
    "\n",
    "loss = distance(style(reference_image) - style(generate_image)) + distance(content(original_image) - content(generated_image))\n",
    "\n",
    "Here distance is a norm fn such as the L2 norm, content is a fn dat takes an image and computes a rep of its content,\n",
    "and style is a fn that takes an image and computes a rep of its style. Minimizing this loss causes style(generated_image)\n",
    "to be close to style(reference_image), and content(generated_image) is close to content(generated_image), 4 style transfer.\n",
    "\n",
    "Deep Cnns offer a way to mathemeticall define style and content fns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 The content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Good candidate: L2 norm btw the activations of an upper layer in a pretrained convnet, computed over the target\n",
    "image, and the activations of the same layer computed over the generated image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 The style loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3 Neural style Transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impl using any pretrained convnet. Here use VGG19. simple variant of VGG16 network with 3 more convolutional layers.\n",
    "Gen process:-\n",
    "    1. Set up network computing VGG19 layer activations for the style-reference image, the target image, and\n",
    "       the genreated image at the same time.\n",
    "    2. Use the layer activations computed over these three images to define the loss fn for minimization 4 style transfer.\n",
    "    3. Set up a gradient descent process to minimize this loss fn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.14 Defining initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "target_image_path = 'img/portrait.jpg'        # Path to the image you want to transform\n",
    "style_reference_image_path = 'img/transfer_style_reference.jpg'    # Path to the style image\n",
    "\n",
    "width, height = load_img(target_img_path).size             # Dimensions\n",
    "img_height = 400                                           # of the \n",
    "img_width = int(width * img_height / height)               # generated picture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.15: Auxilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aux fns needed for loading, pre and postprocessing the images going in and out of the VGG19 convnet.\n",
    "\n",
    "import numpy as np\n",
    "from keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):                    \n",
    "    x[:, :, 0] += 103.939                  # Zero-centering by removing the mean pixel value  \n",
    "    x[:, :, 1] += 116.779                  # from ImageNet. This reverses a transformation\n",
    "    x[:, :, 2] += 123.68                   # done by vgg19.preprocess_input\n",
    "    x = [:, :, ::-1]      # Converts image from 'BGR' 2 'RGB'. also a part of reversal of vgg19.preprocess_input   \n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.16: Loading the pretrained VGG19 network and applying it to the three images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup VGG19 network= input: a batch of three imgs(style_reference_image, the target image and generated_image placeholder) A placeholder= symbolic tensor values provided externally via Numpy arrays. style refernce and target image are constant hence defined using K.constant whereas vals in gen_image_placeholder change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "combination_image = K.placeholder((1, img_height, img_width, 3))   # Placeholder containing the generated image\n",
    "\n",
    "input_tensor = K.concatenate([target_image,                        # Combines the three\n",
    "                              style_reference_image,               # images in a \n",
    "                              combination_image], axis =0)         # single batch\n",
    "\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,                     # Builds the VGG19 network with the batch\n",
    "                    weights='imagenet',                            # of three images as input. The model \n",
    "                    include_top=False)                             # will be loaded with pretrained\n",
    "print('Model loaded.')                                             # ImageNet weights  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
