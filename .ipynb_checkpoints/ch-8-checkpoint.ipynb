{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers:\n",
    "    Text generation with LSTM\n",
    "    Implementing DeepDream\n",
    "    Performing neural style transfer\n",
    "    Variational autoencoders\n",
    "    Understanding generative adversarial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Text generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnns used to generate sequence data. e.g., text generation, same techniques can be genralized to any kind of\n",
    "sequence data: applicaple to a sequences of musical notes in order to generate new music, to timeseries of brush\n",
    "-stroke data(e.g., recorded while an arties paints on an iPad) to generate paintaings stroke by stroke and so on.\n",
    "Sequence data gen not limited to aritstic content generation. usable in speech synthesis and dialogue generation\n",
    "for chatbots. The Smart Reply feature Google 2016\n",
    "\n",
    "character-level neural language model: The output of the model will be a softmax over all possible characters: a \n",
    "        probability distribution for the next character. e.g., take a LSTM layer, feed it strings of N characters\n",
    "        extracted from a text corpus, and train it to predict character N + 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3 The importance of the sampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy sampling = a naive aprroach; always choosing the most likely next element. results are repititive, predictable strings unlike \n",
    "                  coherent language.\n",
    "stochastic sampling = makes slightly more surprising choices: introduces randomness in the sampling process,\n",
    "                  by sampling from the probability distribution for the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing8.1 Reweighting a probability distribution to a different temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Given a temperature value, a new probability distribution is computed from the original one(the softmax output of the model)\n",
    "###by reweighting in the following way\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5): # original distribution is a 1D Numpy array of\n",
    "    distribution = np.log(original_distribution) / temperature     # probability values that must sum to 1. temperature \n",
    "    distribution = np.exp(distribution)                   # is a factor quantifying the entropy of the output distribution.\n",
    "    return distribution / np.sum(distribution)   # Returns a reweighted version of the original distribution. The sum of \n",
    "                        # distribution may no longer be 1, so divide it by its sum to obtain the new distribution.\n",
    "    \n",
    "# Higher temps result in sampling distributions of higher entropy that will genreate more surprising and unstructured\n",
    "# generated data, whereas a lower temp will result in less randomness and much more predicted generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.4 Implementing character-level LSTM text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.2: Downloading and parsing the initial text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.3: vectorizing sequence of characers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract partially overlapping sequences of length maxlen, one-hot encode them, and pack them in a 3D Numpy array x\n",
    "of shape (sequences, maxlen, unique_characters). prepare an array y simulataneously containing the corresponding\n",
    "targets: the one-hot-encoded characters that come after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 60   # Extract sequences of 60 characters\n",
    "step = 3      # Sample a new sequence every three characters\n",
    "sentences = [] # Holds the extracted sequences\n",
    "next_chars = [] # Holds the targets(the follow-up characters)  \n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "chars = sorted(list(set(text)))    # list of unique characters in the corpus\n",
    "print('Unique characters:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)  # Dictionary that maps unique characters to their\n",
    "                                                                  # index in the list \"chars\"  \n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)   # One hot encodes\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)           # the character  \n",
    "for i, sentence in enumerate(sentences):                            # into\n",
    "    for t, char in enumerate(sentence):                             # binary\n",
    "        x[i, t, char_indices[char]] = 1                             # arrays\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING THE NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single LSTM layer followed by a dense classifier and softmax over all possible characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.4: Single layer LSTM model for next-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoded targets, therefore, categorical_crossentropy used as the loss to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.5: Model compilation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE LANGUAGE MODEL AND SAMPLING FROM IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draw from the model a probability distribution for the next character, given the genrated text available so far.\n",
    "2. Reweight the distribution to a certain temperature.\n",
    "3. Sample the next character at random according to the reweighted distribution.\n",
    "4. Add the new character at the end of the available text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.6: Function to sample the next character given the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code used to reweight the original probability distribution coming out of the model and draw a character index from it.\n",
    "# (the sampling function)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.7: Text-generation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):     # Trains the model for 60 epochs\n",
    "    print('epochs', epoch)\n",
    "    model.fit(x, y, batch_size=128, epochs=1)   # Fits the model for one iteration on the data\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)   # Selects a text seed at random \n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('---Generating with seed: \"' + generated_text +'\"')\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:     # Tries a range of different sampling temperatures\n",
    "        print('------temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "        \n",
    "            for i in range(400):     # Generates 400 characters, starting from the seed text\n",
    "                sampled = np.zeros((1, maxlen, len(chars)))    # One-hot encodes the characters generated so far.\n",
    "                for t, char in enumerate(generated_text):\n",
    "                    sampled[0, t, char_indices[char]] = 1.\n",
    "                    \n",
    "                preds = model.predict(sampled, verbose=0)[0]\n",
    "                next_index = sample(preds, temperature)\n",
    "                next_char = chars[next_index]\n",
    "                \n",
    "                generated_text += next_char\n",
    "                generated_text = generated_text[1:]\n",
    "                \n",
    "                sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 DeepDream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artistic image-modification technique that uses the representations learned by convolutional neural networks. \n",
    "Trained on ImageNet, where dog breeds and bird species are vastly over represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Implementing DeepDream in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.8 Loading the pretrained Inception V3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)  # Model isnt trained, so this command disables all training-specific operations.\n",
    "\n",
    "model = inception_v3.InceptionV3(weights='imagenet',  # Builds the V3 network without its convolution base. The\n",
    "                                 include_top=False)   # model will be loaded with pretrained ImageNet weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.9 Setting Up the DeepDream configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_contributions = {   # Dictionary mapping layer names to a coefficient quantifying how much the layer's \n",
    "    'mixed2': 0.2,        # activation contributes to the loass you 'll seek to maximize. Note that the layer names\n",
    "    'mixed3': 3.,         # are hardcoded in the builtin V3. All layers list using model.summary()\n",
    "    'mixed4': 2.,\n",
    "    'mixed5': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.10 Listing the loss to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])   # Creates a dict maps layer names 2 layer insts.\n",
    "\n",
    "loss = K.variable(0.)  # Define loss by adding layer contributions to this scalar variable.\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output    # Retrieves the layer's output\n",
    "    \n",
    "    scaling = K.prod(K.cast(K.Shape(activation), 'float32'))\n",
    "    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling # Adds the L2 norm of the features of\n",
    "            # a layer to the loss. Border artifacts be avoided by only involving non border pixels in the loss,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.11 Gradient-ascent process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream = model.input      # This tensor holds the generated image: the dream.\n",
    "\n",
    "grads = K.gradients(loss, dream)[0]  # Computes the gradients of the dream with regard to the loss.\n",
    "\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)   # Normalizes the gradients (important trick)\n",
    "\n",
    "outputs = [loss, grads]                             # Sets up a Keras function to retrieve the value of the loss \n",
    "fetch_loss_and_grads = K.function([dream], outputs) # and gradients given an input image\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):  # This fn runs a grad asc 4 no(s) of iterations.\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('...Loss value at', i, ':', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.12 Running gradient ascent over different successive scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "step = 0.01          # Gradient ascent step size                        # Playing with these hyperparameters \n",
    "num_octave = 3       # No. of scales at which to run gradient ascent.   # will let you achieve new effects. \n",
    "octave_scale = 1.4   # Size ratio between scales\n",
    "iterations = 20      # Number of ascent steps to run at each scale\n",
    "\n",
    "max_loss = 10.       # If loss > 0, grad ascent process be interrupted to avoud ugly artifacts.\n",
    "\n",
    "base_image_path = '...'     # Path to the image to be used\n",
    "\n",
    "img = preprocess_image(base_image_path)    # Loads the base image into a Numpy array(fn defined next listing)\n",
    "\n",
    "original_shape = img.shape[1:3]\n",
    "successive_shapes = [original_shape]                # Prepares a list of shape tuples defining the different scales\n",
    "for i in range(1, num_octave):                      # at which to run gradient ascent.\n",
    "    shape = tuple([int(dim / (octave_scale ** i))\n",
    "        for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "    \n",
    "successive_shapes = successive_shapes[::-1]     # Reverses the list of shape so they 're in increasing order.\n",
    "    \n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0]) # Resizes Numpy array of the img to the smallest scale.\n",
    "\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape', shape)\n",
    "    img = resize_img(img, shape)      # Scales up the dream image\n",
    "    img = gradient_ascent(img,             \n",
    "                          iterations=iterations,     # Runs gradient \n",
    "                          step=step,                 # ascent\n",
    "                          max_loss=max_loss)         # altering the dream\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)   # Scales up smalled pixaled v of org img.\n",
    "    same_size_original = resize_img(original_img, shape)   # Computes the high quality v of the org img at this size.\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img # The diff btw the two is the detail lost scaling up.\n",
    "    \n",
    "    img += lost_detail   # Reinjects lost detail into the dream\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png' )\n",
    "    \n",
    "save_img(img, fname='final_dream.png')\n",
    "\n",
    "# Note: this code uses aux Numpy fns such as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.13: Auxilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "\n",
    "def preprocess_image(image_path):                       # Util fn 2 open, resize and format pictures into tensors\n",
    "    img = image.load_img(image_path)                    # that Inception V3 can process\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):                                 # Util fn: convert a tensor into a valid image.\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))      # Undoes preprocessing per by inception_v3.preprocess_input\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# Note: Org Inc V3 network trained 2 recognize concepts in imgs of size 299 x 299, and images scaled down by a \n",
    "#       reasonable factor, DeepDream impl produces much better results on images btw 300x300 & 400 x 400. however: any size be run\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepDream consists of running a convnet in reverse to generate inputs based on the representations learned by the\n",
    "network.\n",
    "The results produced are fun and somewhat similar to the visual artifacts induced in humans by the disruption of the\n",
    "visual cortex via phsychedelics.\n",
    "Process not specific to image models or even convnets. Doable for speech, music and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Neural style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consists of applying the style of a reference image to a target image while conserving the target image content.\n",
    "style means textures, colors and visual patterns in the image at various spatial scales and the content is the\n",
    "higher-level macrostructure of the image. \n",
    "define a loss fn to specify what to achieve, and minimize this loss.\n",
    "wat to achieve = conserve the content of the original image while adopting the style of the reference image\n",
    "\n",
    "loss = distance(style(reference_image) - style(generate_image)) + distance(content(original_image) - content(generated_image))\n",
    "\n",
    "Here distance is a norm fn such as the L2 norm, content is a fn dat takes an image and computes a rep of its content,\n",
    "and style is a fn that takes an image and computes a rep of its style. Minimizing this loss causes style(generated_image)\n",
    "to be close to style(reference_image), and content(generated_image) is close to content(generated_image), 4 style transfer.\n",
    "\n",
    "Deep Cnns offer a way to mathemeticall define style and content fns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 The content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Good candidate: L2 norm btw the activations of an upper layer in a pretrained convnet, computed over the target\n",
    "image, and the activations of the same layer computed over the generated image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2 The style loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3 Neural style Transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impl using any pretrained convnet. Here use VGG19. simple variant of VGG16 network with 3 more convolutional layers.\n",
    "Gen process:-\n",
    "    1. Set up network computing VGG19 layer activations for the style-reference image, the target image, and\n",
    "       the genreated image at the same time.\n",
    "    2. Use the layer activations computed over these three images to define the loss fn for minimization 4 style transfer.\n",
    "    3. Set up a gradient descent process to minimize this loss fn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.14 Defining initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "target_image_path = 'img/portrait.jpg'        # Path to the image you want to transform\n",
    "style_reference_image_path = 'img/transfer_style_reference.jpg'    # Path to the style image\n",
    "\n",
    "width, height = load_img(target_img_path).size             # Dimensions\n",
    "img_height = 400                                           # of the \n",
    "img_width = int(width * img_height / height)               # generated picture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.15: Auxilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aux fns needed for loading, pre and postprocessing the images going in and out of the VGG19 convnet.\n",
    "\n",
    "import numpy as np\n",
    "from keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):                    \n",
    "    x[:, :, 0] += 103.939                  # Zero-centering by removing the mean pixel value  \n",
    "    x[:, :, 1] += 116.779                  # from ImageNet. This reverses a transformation\n",
    "    x[:, :, 2] += 123.68                   # done by vgg19.preprocess_input\n",
    "    x = [:, :, ::-1]      # Converts image from 'BGR' 2 'RGB'. also a part of reversal of vgg19.preprocess_input   \n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.16: Loading the pretrained VGG19 network and applying it to the three images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup VGG19 network= input: a batch of three imgs(style_reference_image, the target image and generated_image placeholder) A placeholder= symbolic tensor values provided externally via Numpy arrays. style refernce and target image are constant hence defined using K.constant whereas vals in gen_image_placeholder change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "combination_image = K.placeholder((1, img_height, img_width, 3))   # Placeholder containing the generated image\n",
    "\n",
    "input_tensor = K.concatenate([target_image,                        # Combines the three\n",
    "                              style_reference_image,               # images in a \n",
    "                              combination_image], axis =0)         # single batch\n",
    "\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,                     # Builds the VGG19 network with the batch\n",
    "                    weights='imagenet',                            # of three images as input. The model \n",
    "                    include_top=False)                             # will be loaded with pretrained\n",
    "print('Model loaded.')                                             # ImageNet weights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.17: Content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.18: Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses aux fn 2 compute the Gram matrix of an input matrix: a map of correlations found in org feature matrix.\n",
    "\n",
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.19: Total Variation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total variation loss: operates on the px of gen combination images, added to above two loss components.\n",
    "# ensure continuity in the generated image avoiding overly pixelated results.\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:, :img_height -1, :img_width - 1, :] -\n",
    "        x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_height -1, :img_width - 1, :] -\n",
    "        x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n",
    "# loss two minimize is a weighted avg of above 3 losses. content loss needs only one upper layer-the block5_Conv2\n",
    "# layer , for style loss, list of low and high level layers are used. total variation loss added at end.\n",
    "# Tune the content_weight coefficient depending on the style_reference image and content image.\n",
    "# A higher content_weight means the target content will be more recognizable in the generated image.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.20 Defining the final loss to be minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])  # Dict maps layer names 2 activation tensors\n",
    "content_layer = 'block5_conv2'       # Layer used for content loss\n",
    "style_layers = ['block1_conv1',        # Layers\n",
    "                'block2_conv1',        # used\n",
    "                'block3_conv1',        # for\n",
    "                'block4_conv1',        # style\n",
    "                'block5_conv1',]       # loss\n",
    "\n",
    "total_variation_weight = 1e -4       # Weights in the\n",
    "style_weight = 1.                    # weighted avg of the \n",
    "content_weight = 0.025               # loss components\n",
    "\n",
    "loss = K.variable(0.)     # Define loss by adding all components to this scalar variable.   # Adds\n",
    "layer_features = outputs_dict[content_layer]                                                # the\n",
    "target_image_features = layer_features[0, :, :, :]                                          # cont\n",
    "combination_features = layer_features[2, :, :, :]                                           # ent\n",
    "loss += content_weight * content_loss(target_image_features, combination_features)          # loss\n",
    "\n",
    "for layer_names in style_layers:                                                        \n",
    "    layer_features = outputs_dict[layer_name]             # Adds a style component for each target layer\n",
    "    style_reference_features = layer_features[1, :, :, :]                                       \n",
    "    combination_features = layer_features[2, :, :, :]                                           \n",
    "    s1 = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layers)) * s1\n",
    "\n",
    "loss += total_variation_weight * total_variation_loss(combination_image) # Adds the total variation loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.21: Setting up the gradient-descent process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up gradient-descent process. L-BFGS algorithm used here for optimization. The L-BFGS algorithm comes packaged with \n",
    "SciPy, however 2 slight limitations.\n",
    "1. requirement: pass the value of the loss fn and value of the gradients as two spearate functions.\n",
    "2. Can only be applied to flat vectors, whereas there is a 3D image array.\n",
    "   Python class named Evaluator computes both the loss value and gradients value at once, returns the loss value\n",
    "   when called the first time, and caches the gradients for the next call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, combination_image)[0]   # Gets the gradients of the generated image wrt the loss.\n",
    "\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss, grads]) # fn(vals)=>values[current_loss,current_gradients]\n",
    "\n",
    "class Evaluator(object):     # This class wraps fetch_loss_and_grads in a way to render the losses and gradients\n",
    "                             # via 2 seperate method calls, a reqt of ScipPy optimizer being used.\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "        self.grads_values = None\n",
    "        \n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        \n",
    "        loss.value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "    \n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "    evaluator = Evaluator()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.22: Style-transfer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient-ascent process using SciPy's L-BFGS algorithm, saving the current generated image at each iteration\n",
    "# of the algorithm(here, a single i represents 20 steps of gradient ascent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_1_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "import time\n",
    "\n",
    "result_prefix = 'my_result'\n",
    "iterations = 20\n",
    "\n",
    "x = preprocess_image(target_image_path)    # This is the initial state: the target image.\n",
    "x = x.flatten()  # Image flattened coz scipy.optimize.fmin_l_bfgs_b can only process flat vectors.\n",
    "for i in range(iterations):                                    # Runs L-BFGS optimization\n",
    "    print('Start of iteration', i)                             # over the pixels of the \n",
    "    start_time = time.time()                                   # generated image to minimize the neural\n",
    "    x, min_val, info = fmin_1_bfgs_b(evaluator.loss,           # style loss. Pass the function that computes \n",
    "                                     x,                        # the loss and the function that computes the \n",
    "                                     fprime=evaluator.grads,   # gradients as two separate\n",
    "                                     maxfun=20)                # arguments\n",
    "    print('Current loss value:', min_val)\n",
    "    img = x.copy().reshape((img_height, img_width, 3))                   # Saves\n",
    "    img = deprocess_image(img)                                           # the\n",
    "    fname = result_prefix + '_at_iteration_%d.png' % i                   # current\n",
    "    imsave(fname, img)                                                   # gen\n",
    "    print('Image saved as', fnmae)                                       # rated\n",
    "    end_time = time.time()                                               # image.\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.4: Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-. Style transfer consists of creating a new image that preserves the contents of a target image while also\n",
    "   capturing the style of a reference image.\n",
    "-. Content cal be captured by the high-level activations of a convnet.\n",
    "-. Stell can be captured by the internal correlations of the activations of different layers of a convnet.\n",
    "-. Hence, deep learning allows style transfer to be formulated as an optimization process using a loss defined\n",
    "   with a pretrained convnet.\n",
    "-. Starting from this basic idea, many variants and refinements are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Generating images with variational autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from a latent space of images to create entirely new images or edit existing ones is currently the most popular and successful application of creativeAI. \n",
    "Two main techniques: variational autoencoders(VAEs) and generative adversarial networks(GANs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE Working: \n",
    "    1. An encoder module turns the input samples input_img into two paramters in a latent space of representations,\n",
    "       z_mean and z_log_variance.\n",
    "    2. Sample a random point z from the latent normal distribution that's assumed to generate the input image, via\n",
    "       z = z_mean + exp(z_log_variance) * epsilon, where epsilon is a random tensor of small values.\n",
    "    3. A decoder module maps this point in the latent space back to the original input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coz epsilon is random, the process ensures that every point close to the latent location where input_img(z-mean)\n",
    "can be decoded to sth similar 2 input_img, thus forcing the latent space to be continuously meaningful. \n",
    "The parameters of a VAE are trained via two loss functions: a reconstruction loss forcing decoded samples match initial inputs\n",
    "    2. a regularization loss helping learn well-formed latent spaces and reduce overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_+variance = encode(input_img)     # Encodes the input into a mean and variance parameter.\n",
    "\n",
    "z = z_mean + exp(z_log_variance) * epsilon      # Draws a latent point using a small random epsilon\n",
    "\n",
    "reconstructed_img = decoder(z)                  # Decodes z back to an image\n",
    "\n",
    "model = Model(input_img, reconstructed_img)     # Instantiates the autoencoder models, maps an input image to its reconstruction.\n",
    "\n",
    "# then train the model using the reconstruction loss and the regularization loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.23: VAE encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A smiple convnet mapping the input image x to two vectors, z_mean and z_log_var.\n",
    "\n",
    "import keras \n",
    "from keras import layers\n",
    "from keras import Backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2                        # Dimensionality of the latent space: a 2D plane\n",
    "\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "x = layers.Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.24: Latent-space sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latest_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z=layers.Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.25: VAE decoder network, mapping latent space points to images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = layers.Input(K.int_shape(z)[1:])   # Input where z will be feeded.\n",
    "\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]), activation='relu')(decoder_input)  # Upsamples the input.\n",
    "\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)   # Reshapes z into a feature map shape as the feature map just\n",
    "                                                     # before the last Flatten layer in the encoder model.\n",
    "\n",
    "    x = layers.Conv2DTranspose(32, 3,                       # Uses a Conv2DTranspose \n",
    "                               padding='same',              # layer and Conv2D layer to    \n",
    "                               activation='relu',           # decode z into \n",
    "                               strides=(2, 2))(x)           # a feature map\n",
    "    x = layers.Conv2D(1, 3,                                 # the same\n",
    "                      padding='same',                       # size as the \n",
    "                      activation='sigmoid')(x)              # originial image input.\n",
    "    \n",
    "    decoder = Model(decoder_input, x) # Instantiates the decoder model, turning \"decoder_input\" into the decoded image.\n",
    "    \n",
    "    z_decoded = decoder(z)            # Applies it to z to recover the decoded z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 8.26: Custom layer used to compute the VAE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a custom layer that internally uses the built-in add_loss layer method to create an arbitrary loss\n",
    "\n",
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "    \n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "    \n",
    "    def call(self, inputs):     # Impl custom layers by writing a call method.\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x                # This output not used but the layer must rtrn sth.\n",
    "    \n",
    "y = CustomVariationalLayer()([input_img, z_decoded]) # Calls custom layer on the input and the decoded output\n",
    "                                                     # to obtain the fincal model output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.27: Training the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instantiate and train the model. loss accounted 4 in da custom layer, external loss not specified at compile time\n",
    "(loss=None), training data not passed during training(only pass x_train to the model in fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "vae = Model(input_img, y)\n",
    "vae.compile(optimizer='rmsprop', loss = None)\n",
    "vae.summary()\n",
    "\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_train.shape + (1,))\n",
    "\n",
    "vae.fit(x=x_train, y=None,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))\n",
    "\n",
    "# Once such a model is trained-on MNIST, in this case-decoder network can b used 2 turn arbitrary latent space vectors into images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.28: Sampling a grid of points from the 2D latent sapce and decoding them to images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "n = 15                                   # display a grid of 15 x 15 digits(255 digits total).\n",
    "digit_size = 28                                               # Transforms linearly spaced coordinates\n",
    "figure = np.zeros((digit_size * n, digit_size * n))           # using the SciPy ppf function to produce\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))                 # values of the latent variable z(coz the prior\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))                 # of the latent space is Gaussian\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)   # Repeats z multiple times to form a complete batch.\n",
    "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)      # Decodes the batch into digit images \n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)  # Reshapes da 1st dgt in batch 4rm 28 x 28 x 1 to 28 x 28\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "        \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4.4 Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image generation with dl is done by learning latent spaces that capture statistical information about a dataset of\n",
    "images. By sampling and decoding points from the latent space, never before seen images can be generated.\n",
    "Two tools to do this: VAEs and GANs\n",
    "\n",
    "    VAEs result in higly structured, continuous latent representations. For this reason dey work well for doing       all sort of image editing in latent sapce: face swapping, turning a frowning space into a smiling face and so     on. Also work nicely 4 doing latent-space-based animations, such as animating a walk along a cross section of     the latent space, showing a starting image slowly morphing into different images in a continuous way.\n",
    "    \n",
    "    GANs enable the generation of realistic single-frame images may not induce latent spaces with solid \n",
    "    structures and high continuity.\n",
    "\n",
    "TIP: Large-scale Celeb Faces Attributs (CelebA) dataset. more than 200000 celebrity portraits dataset.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Introduction to generative adversarial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN: a forger network and an expert network, each being trained to best the other. Two parts:\n",
    "        Generator network- Takes an input into random vector (a random point in the latent space),\n",
    "                           and decodes it into a synthetic image.\n",
    "        Discriminator network(adversary)- Takes as input an image (real or synthetic), and predicts \n",
    "                                          whether the image came from the training set or was \n",
    "                                          created by the generator network.\n",
    "While generator network trained to fool the discriminator network and later constantly adapting to\n",
    "the gradually improving capabilities of the former, s higher bar of realism is set for the \n",
    "generated images. \n",
    "Limitations: Unlike VAEs, this latent space has fewer explicit guarantees of meaningful structure\n",
    "             ; in particular, it isnt continuous\n",
    "Getting a GAN to work requires a lot of careful tuning of the model architecture and training params.                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.1 A schematic GAN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "impl deep convolutional GAN(DCGAN); where deepconvnets([generator, discriminators]). \n",
    "Conv2DTranspose layer used for image upsampling in the generator.\n",
    "Train GAN on images from CIFAR10, dataset[50000]: 32 x 32 RGB images, 10 classes(5,000 images/class)\n",
    "    Luckily only using images belonging to the class \"frog.\"\n",
    "    \n",
    "Schematically GAN =>\n",
    "1. A genreator network mapping vectors of shape (latent_dim) to images of shape (32, 32, 3)\n",
    "2. A discriminator network maps images of shape (32, 32, 3) to a binary score estimating the probability \n",
    "   that the image is real.\n",
    "3. A gan network chains the genreator and discriminator together: gan(x) =discriminator(genreator(x)).\n",
    "   maps latent space vectors to the discriminator's assessment of the realism of these latent vectors\n",
    "   as decoded by the generator.\n",
    "4. Train discriminator using eg., of real & fake images along with real/fake labels, like reg image-clasification/\n",
    "5. Gradients of the generator's weights wrt the loss of the gan model, are used to train the generator.\n",
    "   every step, weights of the generator moved in a direction to make the dicriminator more likely\n",
    "   to classify as \"real\" the images decoded by generator. Train the genreator to fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.2: A bag of tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notoriously difficult process of training GANs and tuning GANs impls. Tricks:\n",
    "    * use tanh as the last activation in the generator, instead of sigmoid, more commonly\n",
    "      found in other types of model.\n",
    "    * sample points from the latent space using a normal distribution(Gaussian not uniform)\n",
    "    * Stochasticity is good to introduce robustness. GAN training results in a dynamic\n",
    "      equilibrium,GANS most likely to get stuck everywhere. Inroducing randomness during\n",
    "    * training helps prevent this. randomness in 2 ways:\n",
    "        by using dropout in the discriminator\n",
    "        by adding random noise to the labels for the discriminator.\n",
    "    * Sparse gradients may hinder GAN training. sparsity desirable in DL not GAN. Two things\n",
    "      induce sparsity, max pooling operatiions and RELU activations. \n",
    "        strided convolutions used for downsampling instead of max pooling\n",
    "        LeakyReLU layer instead of a ReLU activations relaxes sparsity constraints by \n",
    "        allowing small negative activation values.\n",
    "    * In generated images, it's common to see checkerboard artifacts caused by unequal coverage\n",
    "      of the pixel space in the generator. Problem Fixture: Use a kernel size that's divisible \n",
    "      by the stride size whenever a strided Conv2DTranspose or Conv2D are used in both\n",
    "      the genrator and the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5.3 The generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "develop a generator model turning a vector(from the latent space--during training it will be sampled at random)\n",
    "into a candidate image. Problem,generator gets stuck with generated images looking like noise. Solution\n",
    "Use dropout on both the discriminator and the generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.29 GAN generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)                       # Transforms the input\n",
    "x = layers.LeakyReLU()(x)                                              # into a 16 x 16 128-\n",
    "x = layers.Reshape((16, 16, 128))(x)                                   # -channel feature map\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)       # Unsamples \n",
    "x = layers.LeakyReLU()(x)                                              # to 32 x 32 \n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)               # Produces\n",
    "generator = keras.models.Model(generator_input, x)       # Instantiates            # a 32 x 32   \n",
    "           # the generator model, mapping inputs of shape(latent_dim)              # 1-channel feature map \n",
    "           # into an image of shape(32, 32, 3)                                     # (shape of a \n",
    "generator.summary()                                                                # CIFAR10 image)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
