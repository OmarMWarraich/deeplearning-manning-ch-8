{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter covers:\n",
    "    Text generation with LSTM\n",
    "    Implementing DeepDream\n",
    "    Performing neural style transfer\n",
    "    Variational autoencoders\n",
    "    Understanding generative adversarial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Text generation with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnns used to generate sequence data. e.g., text generation, same techniques can be genralized to any kind of\n",
    "sequence data: applicaple to a sequences of musical notes in order to generate new music, to timeseries of brush\n",
    "-stroke data(e.g., recorded while an arties paints on an iPad) to generate paintaings stroke by stroke and so on.\n",
    "Sequence data gen not limited to aritstic content generation. usable in speech synthesis and dialogue generation\n",
    "for chatbots. The Smart Reply feature Google 2016\n",
    "\n",
    "character-level neural language model: The output of the model will be a softmax over all possible characters: a \n",
    "        probability distribution for the next character. e.g., take a LSTM layer, feed it strings of N characters\n",
    "        extracted from a text corpus, and train it to predict character N + 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1.3 The importance of the sampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy sampling = a naive aprroach; always choosing the most likely next element. results are repititive, predictable strings unlike \n",
    "                  coherent language.\n",
    "stochastic sampling = makes slightly more surprising choices: introduces randomness in the sampling process,\n",
    "                  by sampling from the probability distribution for the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing8.1 Reweighting a probability distribution to a different temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Given a temperature value, a new probability distribution is computed from the original one(the softmax output of the model)\n",
    "###by reweighting in the following way\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution, temperature=0.5): # original distribution is a 1D Numpy array of\n",
    "    distribution = np.log(original_distribution) / temperature     # probability values that must sum to 1. temperature \n",
    "    distribution = np.exp(distribution)                   # is a factor quantifying the entropy of the output distribution.\n",
    "    return distribution / np.sum(distribution)   # Returns a reweighted version of the original distribution. The sum of \n",
    "                        # distribution may no longer be 1, so divide it by its sum to obtain the new distribution.\n",
    "    \n",
    "# Higher temps result in sampling distributions of higher entropy that will genreate more surprising and unstructured\n",
    "# generated data, whereas a lower temp will result in less randomness and much more predicted generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1.4 Implementing character-level LSTM text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.2: Downloading and parsing the initial text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.3: vectorizing sequence of characers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract partially overlapping sequences of length maxlen, one-hot encode them, and pack them in a 3D Numpy array x\n",
    "of shape (sequences, maxlen, unique_characters). prepare an array y simulataneously containing the corresponding\n",
    "targets: the one-hot-encoded characters that come after each extracted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 60   # Extract sequences of 60 characters\n",
    "step = 3      # Sample a new sequence every three characters\n",
    "sentences = [] # Holds the extracted sequences\n",
    "next_chars = [] # Holds the targets(the follow-up characters)  \n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "chars = sorted(list(set(text)))    # list of unique characters in the corpus\n",
    "print('Unique characters:', len(chars))\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)  # Dictionary that maps unique characters to their\n",
    "                                                                  # index in the list \"chars\"  \n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)   # One hot encodes\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)           # the character  \n",
    "for i, sentence in enumerate(sentences):                            # into\n",
    "    for t, char in enumerate(sentence):                             # binary\n",
    "        x[i, t, char_indices[char]] = 1                             # arrays\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING THE NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single LSTM layer followed by a dense classifier and softmax over all possible characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.4: Single layer LSTM model for next-character prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoded targets, therefore, categorical_crossentropy used as the loss to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.5: Model compilation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE LANGUAGE MODEL AND SAMPLING FROM IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Draw from the model a probability distribution for the next character, given the genrated text available so far.\n",
    "2. Reweight the distribution to a certain temperature.\n",
    "3. Sample the next character at random according to the reweighted distribution.\n",
    "4. Add the new character at the end of the available text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.6: Function to sample the next character given the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code used to reweight the original probability distribution coming out of the model and draw a character index from it.\n",
    "# (the sampling function)\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.7: Text-generation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):     # Trains the model for 60 epochs\n",
    "    print('epochs', epoch)\n",
    "    model.fit(x, y, batch_size=128, epochs=1)   # Fits the model for one iteration on the data\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)   # Selects a text seed at random \n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('---Generating with seed: \"' + generated_text +'\"')\n",
    "    \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:     # Tries a range of different sampling temperatures\n",
    "        print('------temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "        \n",
    "            for i in range(400):     # Generates 400 characters, starting from the seed text\n",
    "                sampled = np.zeros((1, maxlen, len(chars)))    # One-hot encodes the characters generated so far.\n",
    "                for t, char in enumerate(generated_text):\n",
    "                    sampled[0, t, char_indices[char]] = 1.\n",
    "                    \n",
    "                preds = model.predict(sampled, verbose=0)[0]\n",
    "                next_index = sample(preds, temperature)\n",
    "                next_char = chars[next_index]\n",
    "                \n",
    "                generated_text += next_char\n",
    "                generated_text = generated_text[1:]\n",
    "                \n",
    "                sys.stdout.write(next_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 DeepDream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artistic image-modification technique that uses the representations learned by convolutional neural networks. \n",
    "Trained on ImageNet, where dog breeds and bird species are vastly over represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Implementing DeepDream in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.8 Loading the pretrained Inception V3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)  # Model isnt trained, so this command disables all training-specific operations.\n",
    "\n",
    "model = inception_v3.InceptionV3(weights='imagenet',  # Builds the V3 network without its convolution base. The\n",
    "                                 include_top=False)   # model will be loaded with pretrained ImageNet weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.9 Setting Up the DeepDream configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_contributions = {   # Dictionary mapping layer names to a coefficient quantifying how much the layer's \n",
    "    'mixed2': 0.2,        # activation contributes to the loass you 'll seek to maximize. Note that the layer names\n",
    "    'mixed3': 3.,         # are hardcoded in the builtin V3. All layers list using model.summary()\n",
    "    'mixed4': 2.,\n",
    "    'mixed5': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing 8.10 Listing the loss to be maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])   # Creates a dict maps layer names 2 layer insts.\n",
    "\n",
    "loss = K.variable(0.)  # Define loss by adding layer contributions to this scalar variable.\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output    # Retrieves the layer's output\n",
    "    \n",
    "    scaling = K.prod(K.cast(K.Shape(activation), 'float32'))\n",
    "    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling # Adds the L2 norm of the features of\n",
    "            # a layer to the loss. Border artifacts be avoided by only involving non border pixels in the loss,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
